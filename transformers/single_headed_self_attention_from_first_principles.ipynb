{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention From First Principles: Single Headed\n"
      ],
      "metadata": {
        "id": "r3Km6iHOIz4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "E4HWlYVIhd6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Lets code up the self attention operation for transformers using Numpy/Pytorch. We will do this for both single and multi-headed attention. For Multi-Head the computations remain fairly the same\n",
        "- The attention operation relies on 3 matrices Query (Q) , Key (K) , Value (V).\n",
        "- These contain $d_k$ dimensional vectors for each token in the sequence and are used to compute attention scores using the scaled dot-product attention formula"
      ],
      "metadata": {
        "id": "fHd9XlRoR1s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Single Headed Attention"
      ],
      "metadata": {
        "id": "H_94oN2EhPLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0) How to go from Input of Length S to Attention Matrix:"
      ],
      "metadata": {
        "id": "1YV0y9ufQcOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Given a string with N words \"Hi my name is Royston\" (N=4)\n",
        "2. Tokenizer will split this into \"S\" tokens and represent each token using a token ID. We can think of the tokenID as a one-hot encoded vector of size = vocab_size\n",
        "3. Dimensions of S = S x vocab_size\n",
        "4. Embedding Layer takes each one of the tokens and generates a d_model size embedding. (Embedding layer is pretty much a lookup table that maps the tokenID to emb)\n",
        "5. Embedding layer weights (W_emb) = (vocab_size x d_model)\n",
        "6. Therefore to go from S --> X (Input Embedding Matrix) is through S x W_emb --> X [(S x vocab_size) X (vocab_size x d_model) --> (S x d_model)]\n",
        "7. Now to create Q, K, V we do Q = XWq; K= XWk; V=XWv"
      ],
      "metadata": {
        "id": "EcGmWOoyRzbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Create Attention Matrices Directly"
      ],
      "metadata": {
        "id": "UopYR47oKfRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmHXcQ0AIFXA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.randn() --> randn(d0, d1, ..., dn), Returns a sample (or samples) from the \"standard normal\" distribution.\n",
        "# If positive int_like arguments are provided, randn generates an array of shape (d0, d1, ..., dn), filledwith random floats sampled from a univariate \"normal\" (Gaussian)distribution of mean 0 and variance 1.\n",
        "# A single float randomly sampled from the distribution is returned if no argument is provided.\n",
        "\n",
        "seq_len, d_model = 4, 16\n",
        "num_heads = 1\n",
        "d_k = d_model//num_heads\n",
        "\n",
        "# Generates a Matrix of seq_len x d_k. Here d_k = d_model\n",
        "Q = np.random.randn(seq_len, d_k)\n",
        "\n",
        "# Do the same thing for K & V values too\n",
        "K = np.random.randn(seq_len, d_k)\n",
        "V = np.random.randn(seq_len, d_k)"
      ],
      "metadata": {
        "id": "RupHHiE2ITeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query Matrix:Shape S x d_k\\n\", Q)\n",
        "print(\"\\nKey Matrix: S x d_k\\n\", K)\n",
        "print(\"\\nValue Matrix: S x d_k\\n\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi_tQg2dMdea",
        "outputId": "bfc943ed-da60-458b-94b5-e9c4108dc442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Matrix:Shape S x d_k\n",
            " [[-0.15951622  0.98872276 -1.3099051   0.08709159  0.24916526  0.07197099\n",
            "  -2.81090795  0.52460321  0.71404504  0.99795676  1.0005279  -0.10157494\n",
            "   0.3856277  -0.06759229  0.1713508  -1.33010782]\n",
            " [ 0.27011345 -0.5443109   0.52075346 -0.4250707   0.96225342 -0.42046436\n",
            "   0.24899346  0.9761435  -0.37189321 -0.03013046 -0.18577981  0.24505931\n",
            "  -0.25772134 -1.24400364 -2.00156289  0.05747019]\n",
            " [ 0.3886474  -1.85040809 -0.58805571  2.22719094  0.76479855 -0.32956112\n",
            "   1.90235074  0.36685287  0.49176445  0.74700256  0.25997698 -0.37644622\n",
            "  -0.03636313  1.0521244  -1.72450375  0.06511079]\n",
            " [ 0.36060969 -0.07505919  0.04791978  1.2800908  -1.60290889  0.85907832\n",
            "   0.8660755  -1.13681212  1.06663053 -0.0890357   0.72101036  0.21743189\n",
            "  -0.19827866  1.0805751  -2.92718364 -1.94171346]]\n",
            "\n",
            "Key Matrix: S x d_k\n",
            " [[-0.88037719 -1.01550339  0.03704356 -0.23753597 -1.09210438 -0.33763308\n",
            "   0.73684182  1.08469059 -0.91820012  0.98528465  0.16655124  0.91075384\n",
            "   0.02610092 -0.62053999 -1.16462457  2.17571037]\n",
            " [ 1.84881673  0.32085456 -0.55409356 -0.5294875   0.15394953  0.89274241\n",
            "   0.60598529  2.28488779 -0.54405324 -0.84246923  1.09245129 -0.7813695\n",
            "  -1.66458344  1.06161579 -0.49681698 -0.96119617]\n",
            " [ 0.06914866  2.44166558 -0.3274559   0.26704796 -0.9243489  -2.44821575\n",
            "   1.09540578  1.3038608  -1.09637157  1.42142145  1.42031125  1.99123138\n",
            "   0.25689027 -1.93085162  0.62777932 -0.33543525]\n",
            " [ 0.90082173 -0.34086985  0.76892021  0.02695328 -1.04746395  0.23074795\n",
            "   0.13568204  1.42086762  1.07343026 -0.52719955 -1.26458053 -1.55025085\n",
            "  -0.48505038  3.58370342 -0.54355812 -0.08446135]]\n",
            "\n",
            "Value Matrix: S x d_k\n",
            " [[ 1.16623198e-01  4.48318222e-01 -5.81880779e-01  5.50993163e-01\n",
            "  -1.12075754e-03  1.19920417e+00 -1.24745337e+00  1.26222304e+00\n",
            "  -1.13790285e+00 -6.00016089e-02  1.02396324e+00  5.31236972e-01\n",
            "  -4.40120096e-01  4.12892696e-01 -1.18238525e+00 -2.12421852e+00]\n",
            " [ 1.74801201e-01 -2.41210314e-01 -8.99004763e-01  1.44554546e+00\n",
            "  -7.91577545e-01  1.02540081e+00 -6.49194446e-01  1.97692280e+00\n",
            "   2.89121097e+00  1.41207522e+00 -1.31170908e+00 -1.40782467e+00\n",
            "  -4.08926033e-01  9.44147763e-01  1.11868729e+00  9.63622405e-01]\n",
            " [-1.40846902e+00  5.43638542e-01 -1.53338998e-01  1.23302052e-01\n",
            "   1.46549278e+00  1.99531457e-01 -2.62430547e+00  4.60788040e-01\n",
            "  -1.85605679e+00  2.26798255e-01  8.56858714e-01 -9.13714378e-01\n",
            "  -1.78320019e-01 -2.06043355e+00 -5.98288709e-01  1.55832064e+00]\n",
            " [ 1.50231946e+00  9.93832873e-01  1.92709608e+00 -4.72112959e-01\n",
            "   1.23666818e+00  4.61993347e-01  1.93106828e+00 -1.82725198e+00\n",
            "  -1.65299129e-01  1.90322806e+00 -3.05728722e-01  3.19613503e-01\n",
            "  -5.66405951e-01 -1.88834119e-01 -5.52892056e-01 -7.94511344e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q.var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPH6Mw9ujIO1",
        "outputId": "5bc63cdf-8ab4-4cb8-aa13-2d7fce92045a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function ndarray.var>"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing some stats on the distribution of values in the respective matrices\n",
        "print(f\"Q Stats --> Mean = {Q.mean()}, Var = {Q.var()}, Std_Dev = {Q.std()}\")\n",
        "print(f\"K Stats --> Mean = {K.mean()}, Var = {K.var()}, Std_Dev = {K.std()}\")\n",
        "print(f\"V Stats --> Mean = {V.mean()}, Var = {V.var()}, Std_Dev = {V.std()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPAphoFui2sW",
        "outputId": "4207864d-6ecd-402d-a227-0952fc7c8f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q Stats --> Mean = -0.014059075471320182, Var = 1.0161102167626466, Std_Dev = 1.0080229247207857\n",
            "K Stats --> Mean = 0.12302292938779634, Var = 1.3184921068673563, Std_Dev = 1.1482561155366673\n",
            "V Stats --> Mean = 0.09465508751880708, Var = 1.3269897497216636, Std_Dev = 1.151950411138285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Xf83_SQn0wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Implement Attention Formula\n",
        "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$"
      ],
      "metadata": {
        "id": "I-sfrqDuiU5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qk_mat_scaled = np.matmul(Q,K.T)/np.sqrt(d_k)\n",
        "qk_mat_scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5sZ2jJFMg-n",
        "outputId": "e7aa82ae-ef21-4257-c83e-56dc337cc14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.34277989,  0.18069606,  0.67286823, -0.66086783],\n",
              "       [ 1.12348479,  0.57402436,  0.43500528, -0.66116886],\n",
              "       [ 0.88573775,  0.6847196 , -1.01119679,  1.39995509],\n",
              "       [-0.47955637,  0.82402816, -1.03747975,  1.61700606]])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_sum= 0.02759229 -0.00779077 +0.92597297 -0.52447276\n",
        "print(col_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdb0AOdnqCb6",
        "outputId": "f1adfd42-b9a3-4610-aa22-e8eccab714f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.42130173000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_sum = 0.02759229+ 1.3389972-0.96834916-0.17945724\n",
        "print(row_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxsM3qaVp52Y",
        "outputId": "3316c6a1-94e5-4a77-cc0a-857e5239a566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21878309000000015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qk_mat_scaled.shape\n",
        "print(np.sum(qk_mat_scaled, axis = 0)) # Axis = 0, gives us col sums\n",
        "print(np.sum(qk_mat_scaled, axis = 1)) # Axis = 1, gives us row sums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nUMzDVupwkf",
        "outputId": "bf457db9-b7cd-4e6f-c38f-7a01b99796cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.18688627  2.26346818 -0.94080303  1.69492447]\n",
            "[-1.15008343  1.47134557  1.95921564  0.9239981 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mask (If Needed): Decoder Only"
      ],
      "metadata": {
        "id": "Np4tiGqin5UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.tril creates a lower triangle.\n",
        "mask = np.tril(np.ones(shape=(seq_len, seq_len)))\n",
        "\n",
        "# Replace all the slots that are\n",
        "mask[mask==0] = -np.inf\n",
        "\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Z4pBpZoifr",
        "outputId": "0c4cae5e-015b-4d2f-f658-ffe08a7e002b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1., -inf, -inf, -inf],\n",
              "       [  1.,   1., -inf, -inf],\n",
              "       [  1.,   1.,   1., -inf],\n",
              "       [  1.,   1.,   1.,   1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  # Create The exponent\n",
        "  exp_x = np.exp(x)\n",
        "  row_sums = np.sum(exp_x, axis = 1, keepdims=True)\n",
        "  soft = exp_x/row_sums\n",
        "\n",
        "  return soft"
      ],
      "metadata": {
        "id": "mvTKwmm1oiEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Attention Computation: Encoder Block\n",
        "attention_scores = softmax(qk_mat_scaled)\n",
        "print(\"Computed Attention Matrix:\\n\",attention_scores)\n",
        "print(\"Validating Softmax operation. Row Sums= \", np.sum(attention_scores, axis = 1))\n",
        "\n",
        "# Attention Output\n",
        "attention_output = np.matmul(attention_scores, V)\n",
        "print(\"\\n\\nAttention Output:\\n\", attention_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfcfwqwfpLfx",
        "outputId": "bc91a2bd-20be-4f68-ea0a-6c496e8201b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed Attention Matrix:\n",
            " [[0.06635087 0.30442748 0.49800248 0.13121917]\n",
            " [0.44494759 0.25685098 0.22351466 0.07468676]\n",
            " [0.27470607 0.22468143 0.04121355 0.45939896]\n",
            " [0.07466578 0.27495473 0.04273843 0.60764106]]\n",
            "Validating Softmax operation. Row Sums=  [1. 1. 1. 1.]\n",
            "\n",
            "\n",
            "Attention Output:\n",
            " [[-0.44333561  0.35745853 -0.1357813   0.47607709  0.65104129  0.55171797\n",
            "  -1.3339197   0.67528231 -0.14134809  0.78857944  0.05522086 -0.80642508\n",
            "  -0.3168178  -0.73605936 -0.10839245  0.8242018 ]\n",
            " [-0.10582103  0.33326035 -0.38016166  0.60875208  0.21610571  0.87606121\n",
            "  -1.16414312  1.03591902 -0.19089832  0.52883401  0.28738281 -0.3055862\n",
            "  -0.3830236  -0.04841962 -0.41378356 -0.40869053]\n",
            " [ 0.70342766  0.54793132  0.51715043  0.2643419   0.45036158  0.78027984\n",
            "   0.29042886 -0.02952878  0.18457965  1.18447241 -0.11856502 -0.06120534\n",
            "  -0.48033725  0.15388861 -0.35211578 -0.66780141]\n",
            " [ 0.90944541  0.59428003  0.87379705  0.15699441  0.59635156  0.66073211\n",
            "   0.78959652 -0.45281084  0.53022222  1.54994921 -0.43335812 -0.19226325\n",
            "  -0.49709069  0.08762378 -0.1422252  -0.30983144]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply mask if needed (for decoder)\n",
        "masked_qk_mat_scaled = qk_mat_scaled + mask\n",
        "masked_attention_weights = softmax(masked_qk_mat_scaled)\n",
        "print(\"\\nMasked softmax attention weights (for decoder):\\n\", masked_attention_weights)\n",
        "print(\"Sum of each row after masked softmax:\", np.sum(masked_attention_weights, axis=1))\n",
        "\n",
        "# Attention Output\n",
        "attention_output_decoder = np.matmul(masked_attention_weights, V)\n",
        "print(\"\\n\\nAttention Output:\\n\", attention_output_decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qKZCQmFpYJo",
        "outputId": "a68160b9-3ea7-4dfd-983c-f89a329258ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Masked softmax attention weights (for decoder):\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.6340104  0.3659896  0.         0.        ]\n",
            " [0.50814934 0.41561412 0.07623654 0.        ]\n",
            " [0.07466578 0.27495473 0.04273843 0.60764106]]\n",
            "Sum of each row after masked softmax: [1. 1. 1. 1.]\n",
            "\n",
            "\n",
            "Attention Output:\n",
            " [[ 1.16623198e-01  4.48318222e-01 -5.81880779e-01  5.50993163e-01\n",
            "  -1.12075754e-03  1.19920417e+00 -1.24745337e+00  1.26222304e+00\n",
            "  -1.13790285e+00 -6.00016089e-02  1.02396324e+00  5.31236972e-01\n",
            "  -4.40120096e-01  4.12892696e-01 -1.18238525e+00 -2.12421852e+00]\n",
            " [ 1.37915743e-01  1.95957947e-01 -6.97944860e-01  8.78390003e-01\n",
            "  -2.90419724e-01  1.13559395e+00 -1.02849683e+00  1.52379572e+00\n",
            "   3.36710915e-01  4.78763205e-01  1.69131453e-01 -1.78439429e-01\n",
            "  -4.28703393e-01  6.07326527e-01 -3.40216627e-01 -9.94100842e-01]\n",
            " [ 2.45350475e-02  1.69007318e-01 -6.81011443e-01  8.90176039e-01\n",
            "  -2.17836221e-01  1.05075745e+00 -1.10377495e+00  1.49816372e+00\n",
            "   4.81904171e-01  5.73678935e-01  4.04853726e-02 -3.84822513e-01\n",
            "  -4.07196672e-01  4.45131974e-01 -1.81497517e-01 -5.60124194e-01]\n",
            " [ 9.09445411e-01  5.94280035e-01  8.73797049e-01  1.56994408e-01\n",
            "   5.96351560e-01  6.60732105e-01  7.89596518e-01 -4.52810837e-01\n",
            "   5.30222219e-01  1.54994921e+00 -4.33358123e-01 -1.92263253e-01\n",
            "  -4.97090687e-01  8.76237834e-02 -1.42225200e-01 -3.09831439e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Modularizing: Creating Final Attention Functions/Class"
      ],
      "metadata": {
        "id": "uZx4FNDvDTzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base Version: Creating Independent Functions"
      ],
      "metadata": {
        "id": "vI-H7xl5IkyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  # Create The exponent\n",
        "  exp_x = np.exp(x)\n",
        "  row_sums = np.sum(exp_x, axis = 1, keepdims=True)\n",
        "  soft = exp_x/row_sums\n",
        "\n",
        "  return soft\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, d_k, mask=None):\n",
        "  qk_final= np.matmul(Q, K.T)/np.sqrt(d_k)\n",
        "  seq_len = Q.shape[0]\n",
        "  if mask:\n",
        "    mask = np.tril(np.ones(shape=(seq_len, seq_len)))\n",
        "    qk_final = qk_final + mask\n",
        "\n",
        "  # Attention Scores\n",
        "  attention_scores = softmax(qk_final)\n",
        "\n",
        "  # Return Output of Computation\n",
        "  output = np.matmul(attention_scores, V)\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "UlHihdaCCnTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Intermediate Level: Creating Class + Methods with Type Hinting"
      ],
      "metadata": {
        "id": "G0GLz849In1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import numpy as np\n",
        "\n",
        "class ScaledDotProductAttentions:\n",
        "  def __init__(self, Q: np.ndarray, K: np.ndarray,V: np.ndarray, d_k: int) -> None:\n",
        "    \"\"\"\n",
        "        Initializes the ScaledDotProductAttentions class.\n",
        "\n",
        "        Args:\n",
        "            Q (np.ndarray): The Query matrix.\n",
        "            K (np.ndarray): The Key matrix.\n",
        "            V (np.ndarray): The Value matrix.\n",
        "            d_k (float): The dimension of the key vectors (used for scaling).\n",
        "    \"\"\"\n",
        "    self.Q = Q\n",
        "    self.K = K\n",
        "    self.V = V\n",
        "    self.d_k = d_k\n",
        "    self.attention_scores = None\n",
        "    self.attention_output = None\n",
        "\n",
        "  def softmax(self, x) -> np.ndarray:\n",
        "    \"\"\"\n",
        "        Computes the softmax function along the rows of a matrix.\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): The input matrix.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The matrix with softmax applied along each row.\n",
        "        \"\"\"\n",
        "    # Create The exponent matrix\n",
        "    exp_x = np.exp(x)\n",
        "    # For large values of x, exp(x) can overflow. A more robust pattern will:\n",
        "    # Subtract the maximum value from each row for numerical stability\n",
        "    # exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "\n",
        "    # Compute Row Sums of Exponent Matrix\n",
        "    row_sums = np.sum(exp_x, axis = 1, keepdims=True)\n",
        "\n",
        "    # Compute Softmax Matrix\n",
        "    soft = exp_x/row_sums\n",
        "\n",
        "    return soft\n",
        "\n",
        "\n",
        "  def compute_single_head_attention(self, mask: Optional[np.ndarray] = None):\n",
        "    \"\"\"\n",
        "        Computes the scaled dot-product attention.\n",
        "\n",
        "        Args:\n",
        "            mask (Optional[np.ndarray]): An optional mask matrix to apply before softmax.\n",
        "                                         Typically used for masking future tokens in decoders.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The attention output matrix.\n",
        "    \"\"\"\n",
        "    qk_final = np.matmul(self.Q, self.K.T)/np.sqrt(self.d_k)\n",
        "    seq_len = self.Q.shape[0]\n",
        "    if mask:\n",
        "      mask = np.tril(np.ones(shape=(seq_len, seq_len)))\n",
        "      qk_final = qk_final + mask\n",
        "\n",
        "    # Attention Scores\n",
        "    self.attention_scores = self.softmax(qk_final)\n",
        "\n",
        "    # Attenion Output\n",
        "    self.attention_output = np.matmul(self.attention_scores, self.V)\n",
        "\n",
        "    return self.attention_output"
      ],
      "metadata": {
        "id": "GQPDsuh9IX9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Advanced Class: Pydantic + DataClass"
      ],
      "metadata": {
        "id": "948CqHmlJldJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field, ConfigDict, model_validator\n",
        "# Import dataclasses and NpNDArray (if you want pydantic_numpy validation)\n",
        "from dataclasses import dataclass\n",
        "# from pydantic_numpy import NpNDArray\n",
        "\n",
        "\n",
        "# Install pydantic, pydantic-numpy, if not installed\n",
        "# pip install pydantic \"pydantic-numpy>=1.2.0\"\n",
        "\n",
        "# 1. Define the data structure using a dataclass\n",
        "@dataclass\n",
        "class AttentionInputData:\n",
        "    \"\"\"\n",
        "    Dataclass defining the structure and basic types of attention inputs.\n",
        "    \"\"\"\n",
        "    Q: np.ndarray\n",
        "    K: np.ndarray\n",
        "    V: np.ndarray\n",
        "    d_k: int\n",
        "\n",
        "# 2. Define a Pydantic model that uses the dataclass\n",
        "class AttentionInputs(BaseModel):\n",
        "    \"\"\"\n",
        "    Pydantic model for validating the AttentionInputData dataclass.\n",
        "    \"\"\"\n",
        "    # Pydantic can validate dataclasses directly\n",
        "    data: AttentionInputData\n",
        "\n",
        "    # Optional configuration for the model\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True) # Still potentially needed for NumPy arrays within dataclass\n",
        "\n",
        "    # You can still add validators that operate on the nested dataclass data\n",
        "    @model_validator(mode='after')\n",
        "    def check_matrix_shapes(self) -> 'AttentionInputs':\n",
        "        \"\"\"\n",
        "        Validator to check that the shapes of Q, K, and V within the dataclass are compatible.\n",
        "        \"\"\"\n",
        "        q_shape = self.data.Q.shape\n",
        "        k_shape = self.data.K.shape\n",
        "        v_shape = self.data.V.shape\n",
        "        d_k_val = self.data.d_k # Access d_k from the dataclass\n",
        "\n",
        "        # Basic shape checks for attention (seq_len, d_k)\n",
        "        if q_shape[1] != d_k_val or k_shape[1] != d_k_val:\n",
        "            # Use a more informative error message\n",
        "            raise ValueError(f\"Last dimension of Q ({q_shape[1]}) or K ({k_shape[1]}) must match d_k ({d_k_val}). Q shape: {q_shape}, K shape: {k_shape}\")\n",
        "\n",
        "        if q_shape[0] != k_shape[0] or q_shape[0] != v_shape[0]:\n",
        "             raise ValueError(f\"Sequence lengths of Q ({q_shape[0]}), K ({k_shape[0]}), and V ({v_shape[0]}) must match.\")\n",
        "\n",
        "        # You could add more checks, e.g., d_k > 0 here, or as a separate field validator\n",
        "        if d_k_val <= 0:\n",
        "             raise ValueError(f\"d_k must be positive, but got {d_k_val}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "# You could also potentially use pydantic_numpy types directly in the dataclass\n",
        "# and then just validate the dataclass itself if you want stronger type checking\n",
        "# at the dataclass level and Pydantic to pick that up.\n",
        "\n",
        "## @dataclass\n",
        "# class AttentionInputDataStrict:\n",
        "#      Q: NpNDArray[np.float64]\n",
        "#      K: NpNDArray[np.float64]\n",
        "#      V: NpNDArray[np.float64]\n",
        "#      d_k: float\n",
        "\n",
        "# class AttentionInputsStrict(BaseModel):\n",
        "#      data: AttentionInputDataStrict\n",
        "#      model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "\n",
        "class ScaledDotProductAttentions:\n",
        "    def __init__(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray, d_k: float) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the ScaledDotProductAttentions class with Pydantic validation\n",
        "        using a nested dataclass.\n",
        "\n",
        "        Args:\n",
        "            Q (np.ndarray): The Query matrix.\n",
        "            K (np.ndarray): The Key matrix.\n",
        "            V (np.ndarray): The Value matrix.\n",
        "            d_k (float): The dimension of the key vectors (used for scaling).\n",
        "        \"\"\"\n",
        "        # Create the dataclass instance first\n",
        "        input_data = AttentionInputData(Q=Q, K=K, V=V, d_k=d_k)\n",
        "\n",
        "        # Validate the dataclass instance using the Pydantic model\n",
        "        validated_inputs = AttentionInputs(data=input_data)\n",
        "\n",
        "        # Assign data from the validated dataclass instance\n",
        "        self.Q = validated_inputs.data.Q\n",
        "        self.K = validated_inputs.data.K\n",
        "        self.V = validated_inputs.data.V\n",
        "        self.d_k = validated_inputs.data.d_k\n",
        "\n",
        "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the softmax function along the rows of a matrix.\n",
        "        \"\"\"\n",
        "        # Add numerical stability\n",
        "        x = x - np.max(x, axis=1, keepdims=True)\n",
        "        exp_x = np.exp(x)\n",
        "        row_sums = np.sum(exp_x, axis=1, keepdims=True)\n",
        "        soft = exp_x / row_sums\n",
        "        return soft\n",
        "\n",
        "    def compute(self, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the scaled dot-product attention.\n",
        "        \"\"\"\n",
        "        qk_mat_scaled = np.matmul(self.Q, self.K.T) / np.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "             if mask.shape != qk_mat_scaled.shape:\n",
        "                 print(\"Warning: Provided mask shape does not match QK^T shape. Generating default tril mask.\")\n",
        "                 seq_len = self.Q.shape[0]\n",
        "                 mask = np.tril(np.ones(shape=(seq_len, seq_len)))\n",
        "                 mask[mask == 0] = -np.inf\n",
        "\n",
        "             qk_mat_scaled = qk_mat_scaled + mask\n",
        "\n",
        "        attention_mat = self.softmax(qk_mat_scaled)\n",
        "        attention_output = np.matmul(attention_mat, self.V)\n",
        "        return attention_output\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "seq_len, d_model = 4, 6\n",
        "num_heads = 1\n",
        "d_k_val = float(d_model // num_heads)\n",
        "\n",
        "Q_val = np.random.randn(seq_len, int(d_k_val)).astype(np.float64)\n",
        "K_val = np.random.randn(seq_len, int(d_k_val)).astype(np.float64)\n",
        "V_val = np.random.randn(seq_len, d_model).astype(np.float64) # V dimension can be different\n",
        "\n",
        "try:\n",
        "    # Pass the raw data to the class constructor\n",
        "    attention_module = ScaledDotProductAttentions(Q_val, K_val, V_val, d_k_val)\n",
        "    print(\"Attention module created successfully with valid inputs using dataclass.\")\n",
        "    attention_output = attention_module.compute()\n",
        "    print(\"Attention Output:\\n\", attention_output)\n",
        "except Exception as e:\n",
        "    print(f\"Error creating attention module: {e}\")\n",
        "\n",
        "# Example with invalid inputs (uncomment to test)\n",
        "# try:\n",
        "#     # Mismatched d_k\n",
        "#     invalid_Q = np.random.randn(seq_len, 5).astype(np.float64)\n",
        "#     invalid_K = np.random.randn(seq_len, 5).astype(np.float64)\n",
        "#     invalid_V = np.random.randn(seq_len, d_model).astype(np.float64)\n",
        "#     # Pydantic validation will catch this shape mismatch via the model_validator\n",
        "#     attention_module_invalid_dk = ScaledDotProductAttentions(invalid_Q, invalid_K, invalid_V, d_k_val)\n",
        "# except Exception as e:\n",
        "#      print(f\"\\nSuccessfully caught error with invalid d_k via dataclass/Pydantic: {e}\")\n",
        "\n",
        "# try:\n",
        "#     # Mismatched sequence length\n",
        "#     invalid_Q_len = np.random.randn(3, int(d_k_val)).astype(np.float64)\n",
        "#     invalid_K_len = np.random.randn(4, int(d_k_val)).astype(np.float64)\n",
        "#     invalid_V_len = np.random.randn(4, d_model).astype(np.float64)\n",
        "#     # Pydantic validation will catch this shape mismatch via the model_validator\n",
        "#     attention_module_invalid_len = ScaledDotProductAttentions(invalid_Q_len, invalid_K_len, invalid_V_len, d_k_val)\n",
        "# except Exception as e:\n",
        "#      print(f\"\\nSuccessfully caught error with invalid sequence length via dataclass/Pydantic: {e}\")"
      ],
      "metadata": {
        "id": "2vBaZmbeJskA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}